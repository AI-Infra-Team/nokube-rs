{
	"nodes":[
		{"id":"2646f8feb06d5f3b","type":"group","x":-160,"y":360,"width":3960,"height":4180,"label":"集群侧"},
		{"id":"7c01f0b4dd6ebad7","type":"group","x":380,"y":390,"width":3380,"height":4130,"label":"general"},
		{"id":"38790d011e928eb9","type":"group","x":1500,"y":3280,"width":2170,"height":1240,"label":"exporter"},
		{"id":"1992741a2aa2fe3b","type":"group","x":-80,"y":-650,"width":2809,"height":910,"label":"控制侧"},
		{"id":"0520f5a5ed0e5699","type":"group","x":3820,"y":1680,"width":1520,"height":1660,"label":"监控面板"},
		{"id":"029a051cde4f5915","type":"group","x":-1280,"y":260,"width":820,"height":1630,"label":"集群静态配置+集群actor动态配置"},
		{"id":"689775cc9b14b219","type":"group","x":1880,"y":-597,"width":780,"height":805,"label":"k8s原生命令"},
		{"id":"30cdd7fa9b891781","type":"group","x":-140,"y":390,"width":520,"height":1050,"label":"service agent"},
		{"id":"9cf48181aa1561ab","type":"group","x":500,"y":-565,"width":880,"height":402,"label":"new-or-update"},
		{"id":"b1d202983b97c1eb","type":"group","x":660,"y":-960,"width":700,"height":280,"label":"示例"},
		{"id":"8c4790e1a4bce75f","type":"group","x":420,"y":420,"width":346,"height":460,"label":"k8s"},
		{"id":"cli-monitor","type":"text","text":"## CLI: monitor\n\n用法\n- `nokube monitor --cluster <name>`\n\n说明\n- 启动 GreptimeDB + Grafana，自动配置数据源与仪表盘\n\n示例\n- `nokube monitor --cluster home-cluster`","x":520,"y":-142,"width":360,"height":362},
		{"id":"cli-new-or-update","type":"text","text":"## CLI: new-or-update\n\n用法\n- `nokube new-or-update [config.yaml]`\n\n说明\n- 未提供文件时生成模板 `cluster-config-template.yaml` 并提示二次执行\n- 持久化集群配置到 etcd 并触发部署/更新\n\n示例\n- `nokube new-or-update ./home-cluster.yaml`","x":520,"y":-545,"width":360,"height":362},
		{"id":"n-distribution-flow","type":"text","text":"## 分发流程 (基于 HTTP)\n1) 本地构建 & 打包\n2) 上传到 Head HTTP 目录\n3) 启动 http.server 容器 (python:3.10-slim)\n4) 节点下载: `http://<head>:<port>/releases/nokube/...`\n   - cmdagent 通过 curl 下载 `bin/`+`lib/` 后执行\n   - service agent 在容器内使用相同来源获取依赖\n5) 不再预分发到各节点 (无 SSH 逐点拷贝)\n","x":920,"y":-540,"width":440,"height":315},
		{"id":"gitops-extension","type":"text","text":"## GitOps 扩展\n\n**examples/gitops/**\n- 自动化部署\n- 配置同步\n\n🔧 **ConfigMap 挂载支持**\n- YAML内嵌配置文件\n- 自动文件系统映射\n- /etc/config 标准挂载","x":700,"y":-960,"width":230,"height":190},
		{"id":"architecture-principles","type":"text","text":"## 架构原则\n\n### 🔧 最小变更原则\n- 文件分布即模块规划\n- 严格错误处理\n- 无默认值策略\n\n### 🏗️ 容器化运行\n- Service Agent 使用 --pid\n- 观测宿主机信息\n- 统一进程内逻辑\n\n### 📊 监控绑定\n- 集群部署监控\n- 节点 IP 连接\n- 禁用 localhost","x":1070,"y":-920,"width":250,"height":200},
		{"id":"docker-config-example","type":"text","text":"## ConfigMap 挂载示例\n\n**GitOps YAML结构:**\n```yaml\nspec:\n  configMap:\n    data:\n      requirements.txt: \"flask==2.3.2\\nrequests==2.31.0\"\n      webhook-server.py: \"#!/usr/bin/env python3\\n...\"\n  webhookDeployment:\n    containerSpec:\n      image: python:3.10-slim\n      command: [\"/bin/bash\"]\n      args: [\n        \"-c\", \n        \"pip install -r /etc/config/requirements.txt && python /etc/config/webhook-server.py\"\n      ]\n```\n\n**生成的Docker命令:**\n```bash\ndocker run -d \\\n  --name nokube-pod-{deployment-name} \\\n  -v {workspace}:/pod-workspace \\\n  -v {workspace}/configmaps/{deployment-name}:/etc/config:ro \\\n  -e FLASK_PORT=8080 \\\n  python:3.10-slim \\\n  /bin/bash -c \"pip install -r /etc/config/requirements.txt && python /etc/config/webhook-server.py\"\n```\n\n**文件系统结构:**\n```\n{workspace}/\n├── configmaps/\n│   └── {deployment-name}/\n│       ├── requirements.txt\n│       └── webhook-server.py\n└── ...\n```\n\n**动态变量说明:**\n- `{workspace}` - 工作空间路径 (默认: /opt/devcon/pa/nokube-workspace)\n- `{deployment-name}` - 部署名称 (如: gitops-webhook-server-home-cluster)\n- `{cluster-name}` - 集群名称","x":2170,"y":430,"width":940,"height":880},
		{"id":"d8d98b5f9f5121be","type":"text","text":"k8s actor \n对应etcd 某个配置\n此外这个actor的alive应该冗余一个key，并且按照这个actor的粒度设置lease\n定期向 the proxy发出请求，保活自己的lease以及检查自己关联组件","x":441,"y":660,"width":300,"height":195},
		{"id":"63a7919ba26e1c77","type":"text","text":"the_proxy\n特殊actor协程，一个agent一个，用于聚合所有 k8s actor 一段时间内的请求（一般就是校验其从属以及管控的actor是否alive）","x":441,"y":445,"width":360,"height":180},
		{"id":"807d8c1bd92464b0","type":"text","text":"## 📝 OTLP 日志收集 (实现同步)\n\n**Endpoint (约定):**\n- OTLP Logs: `http://{head}:{port}/v1/otlp/v1/logs（port 来自 ClusterConfig.monitoring.greptimedb.port）`\n\n**Headers:**\n- `X-Greptime-DB-Name=public`\n- `X-Greptime-Log-Table-Name=opentelemetry_logs`\n- `X-Greptime-Log-Extract-Keys=cluster_name,node_name,source,source_id,container,pod,root_actor,container_path`\n\n**Collector 行为:**\n- 无失败重试，失败仅 error log\n- 外层超时保护（固定 5s）；内部 `force_flush()` 放入 `spawn_blocking`\n- actor 日志设置 `scope_name=container_path` (cluster/root_actor/pod/container)；其他来源使用 `nokube-rs`\n\n**Greptime 表:**\n- 表: `opentelemetry_logs`\n- 列: timestamp, severity_text, body, scope_name, log_attributes(JSON) 等\n\n**Grafana 日志面板:**\n- 数据源: MySQL (greptimemysql)\n- 变量: `container_path`（来自 `scope_name`）\n- SQL 使用 `${container_path:sqlstring}` 可选筛选\n- 日志一行展示（隐藏 Labels/Details）\n\n**Actor 面板联动:**\n- Containers 表的 Container 列跳转到日志面板，链接携带 `var-container_path=<full-path>`","x":2950,"y":3295,"width":700,"height":900},
		{"id":"docker-ops-module","type":"text","text":"## 🐳 Docker 操作模块\n\n**src/agent/general/docker_runner.rs**\n\n### 核心功能:\n🏗️ **容器生命周期管理**\n- 创建、启动、停止、删除\n- 健康检查和监控\n- 错误处理和恢复\n\n🗂️ **卷挂载管理**\n- ConfigMap 文件挂载\n- Secret 安全挂载\n- 工作空间目录绑定\n- 权限和安全控制\n\n🌐 **网络和端口**\n- 端口映射和转发\n- 网络模式配置\n- 容器间通信\n\n⚙️ **GitOps ConfigMap 处理**\n```rust\n// 1. 解析YAML内嵌配置\nlet configmap_data = spec.get(\"configMap\")\n  .and_then(|cm| cm.get(\"data\"));\n\n// 2. 创建主机文件系统\nlet config_dir = format!(\n  \"{}/configmaps/{}\", \n  workspace, deployment_name\n);\n\n// 3. 写入配置文件\nfor (filename, content) in configmap_data {\n  let file_path = format!(\n    \"{}/{}\", config_dir, filename\n  );\n  std::fs::write(&file_path, content)?;\n}\n\n// 4. 挂载到容器\nconfig = config.add_volume(\n  config_dir, \n  \"/etc/config\".to_string(), \n  true // read-only\n);\n```\n\n🔒 **安全特性**\n- 输入验证和清理\n- 命令注入防护\n- 权限隔离\n- 资源限制","x":1610,"y":430,"width":520,"height":970},
		{"id":"the-proxy-keepalive-task","type":"text","text":"## the_proxy Keepalive [独立task]\n归属: the_proxy\n触发: tokio::spawn (periodic_keepalive)\n周期: keepalive_interval (默认 5s)\n职责: 续租 alive key, 超时标记 Dead\n关键代码: src/k8s/the_proxy.rs::periodic_keepalive","x":960,"y":430,"width":520,"height":300},
		{"id":"the-proxy-alive-task","type":"text","text":"## the_proxy Alive 消费 [独立task]\n归属: the_proxy\n触发: tokio::spawn (handle_alive_requests)\n模式: 异步通道循环\n职责: 接收 ActorAliveRequest, 写 `/nokube/{cluster}/actors/{path}/alive` 带租约条目\n关键代码: src/k8s/the_proxy.rs::handle_alive_requests","x":960,"y":758,"width":520,"height":320},
		{"id":"cluster-config","type":"text","text":"## 集群配置文件\n\n**cluster-config.yaml**\n\n```yaml\ncluster_name: home-cluster\n\nnodes:\n  - ssh_url: \"10.126.126.234:2222\"\n    name: \"pinghu-container\"\n    role: \"head\"\n    workspace: \"/opt/devcon/pa/nokube-workspace\"\n    storage:\n      type: \"local\"\n      path: \"/data/ray/head\"\n    users:\n      - userid: \"pa\"\n        password: \"74123\"\n```\n\n🔧 **Workspace配置来源:**\n1. **集群配置文件** - 每个节点的workspace字段\n2. **代码硬编码** - ServiceAgent中的默认路径\n3. **动态传递** - 通过ClusterConfig传递给Docker操作模块\n\n📂 **实际路径构建:**\n- 基础路径: `{node.workspace}`\n- ConfigMap路径: `{workspace}/configmaps/{deployment-name}`\n- 存储路径: `{workspace}/{storage.path}`","x":-1200,"y":1130,"width":720,"height":740},
		{"id":"k8s-abstraction","type":"text","text":"## Actor 模拟层\nk8s中的daemonset、deployment、pod 统一称作actor，为某个agent里的一个协程所管控\n\n**src/k8s/**\n- objects.rs\n- controllers.rs\n- storage.rs\n- heartbeat.rs\n\n📦 Pod 管理\n⚖️ DaemonSet/Deployment\n🔐 Config/Secret 存储\n💓 生命周期监控","x":-125,"y":970,"width":370,"height":430},
		{"id":"actor-lifecycle","type":"text","text":"## Actor 协程生命周期与治理\n\n### 角色与路径\n- ServiceAgent: `service/{node}/agent`\n- KubeController: `service/{node}/kubecontroller`\n- DeploymentActor: `service/{node}/deployment/{name}`\n- DaemonSetActor: `service/{node}/daemonset/{name}`\n\n> 统一 ActorPath 便于跨节点协同\n\n### 存活与父子关系\n- 存活键: `/nokube/{cluster}/actors/{actor_path}/alive` (etcd 带租约)\n  - `lease_ttl=15s`, keepalive `5s`\n  - 值包含: `last_alive`, `node`, `version`\n- 父键: `/nokube/{cluster}/actors/{actor_path}/parent` → 父ActorPath\n- 子集合: `/nokube/{cluster}/actors/{actor_path}/children` → 子ActorPath列表\n\n### 孤儿回收（中心调度）\n- KubeController 每 15s 扫描 deployments/daemonsets，逐个指派 pod.cleanup_if_orphaned()\n- cleanup_if_orphaned() 由控制面触发：校验父级 etcd 键与 alive 租约，必要时停止容器并清理 `/pods`、`/events`\n- Actor 不再自启伴随协程，自身仅负责业务逻辑与心跳上报\n\n### 心跳判定与可观测性\n- Deployment/DaemonSet/Pod Actor 统一写入 `lease_ttl=15s` 的 alive 心跳；TheProxy 5s keepalive 确保租约续约\n- `PodDescription` 读取 alive 信息，若 `last_alive` 超过 `2 * lease_ttl` (默认 30s) 判定为 Dead，CLI/Grafana 会隐藏这些容器\n- Freshness 规则与 Dashboard 的 `count_over_time(metric[30s]) > 0` 对齐\n\n### 信号通道\n- `/nokube/{cluster}/actors/{actor_path}/signal`: `shutdown|drain|restart`\n- 由父或控制面写入；子消费后写 `/ack`\n\n### 本地容器命名\n- 统一为 `nokube-pod-{deploymentName}`，便于跨模块回收\n","x":420,"y":1520,"width":770,"height":1240},
		{"id":"agent-system","type":"text","text":"## Agent 系统\n\n**src/agent/**\n- general/ 通用功能\n- master_agent/ 主代理\n- service_agent/ 服务代理\n\n🤖 远程命令执行\n📈 监控数据收集\n🐳 容器管理","x":-400,"y":-50,"width":300,"height":300},
		{"id":"nokube-core","type":"text","text":"# Nokube-rs Core\n\n分布式服务管理器\n- 集群初始化\n- 部署控制\n- 配置管理","x":-400,"y":-275,"width":200,"height":100},
		{"id":"remote-control","type":"text","text":"## Remote Control\n\n**src/remote_ctl/**\n- ssh_manager.rs\n- deployment_controller.rs\n\n🔗 SSH 连接管理\n🚀 分布式部署\n\n### 🐳 Docker镜像构建与分发\n**prepare_dependencies()新增功能:**\n1. 构建包含Docker的nokube镜像\n   ```dockerfile\n   FROM ubuntu:22.04\n   RUN apt-get install docker.io python3-pip\n   COPY target/nokube /usr/local/bin/nokube\n   ```\n2. 导出镜像为tar包: `docker save -o nokube-image.tar`\n3. SSH上传tar包到各节点\n4. 远程加载镜像: `docker load -i nokube-image.tar`\n\n**优势:**\n✅ 避免每次容器启动重新下载\n✅ 镜像一次构建，多节点复用\n✅ 离线环境支持\n✅ 网络带宽节省\n\n### 📂 文件上传接口保障\n- 自动创建远程父目录: `mkdir -p <parent>`\n- 自动设置目录归属为当前SSH用户: `chown -R {user}:{user} <dir>`\n- 单文件上传: `upload_file` / `force_upload_file`\n- 目录上传: `upload_directory` (递归处理并统一权限)\n- 调用方无需额外 mkdir/chown，接口已内建","x":-29,"y":-630,"width":510,"height":860},
		{"id":"9213010bd51b4eba","type":"text","text":"docker启动，稳定隔离运行","x":-100,"y":430,"width":160,"height":90},
		{"id":"service-agent-summary","type":"text","text":"## ServiceModeAgent (容器/调度中枢)\n\n职责\n- 管理 TheProxy、KubeController、Exporter 等子组件生命周期\n- 启动 / 监护本地系统服务 (Grafana、GreptimeDB、HTTP Server)\n- 初始化阶段写入 `/nokube/{cluster}/pods|events` 状态 (绑定服务)","x":-100,"y":535,"width":440,"height":345},
		{"id":"etcd-storage","type":"text","text":"## etcd 分布式存储\n\n### 数据结构:\n**业务数据:**\n- `/nokube/{cluster}/pods/{name}` - Pod状态\n- `/nokube/{cluster}/events/pod/{name}` - Pod事件\n- `/nokube/{cluster}/deployments/{name}` - Deployment配置\n- `/nokube/{cluster}/daemonsets/{name}` - DaemonSet配置\n- `/nokube/{cluster}/configmaps/{name}` - ConfigMap\n- `/nokube/{cluster}/secrets/{name}` - Secret\n- `cluster/{name}` - 集群元数据\n- `k8s/configmap/{ns}/{name}` - Actor ConfigMap\n- `k8s/secret/{ns}/{name}` - Actor Secret\n\n**Actor存活监控:**\n- `/nokube/{cluster}/actors/{actor-path}/alive` - Actor存活状态\n  - 包含lease_ttl、last_alive时间戳\n  - the_proxy定期更新\n  - 超时标记为Dead\n\n### 写入者:\n- ConfigManager (集群配置)\n- ServiceAgent (Pod状态/事件)\n- ActorStorage (Actor资源)\n- **TheProxy (Actor存活状态)**\n\n### 读取者:\n- ServiceAgent (部署配置)\n- Exporter (集群配置)\n- CLI (Pod状态查询)\n- **TheProxy (Actor存活监控)**","x":-1260,"y":280,"width":680,"height":760},
		{"id":"nokube-config","type":"text","text":"## Nokube 全局配置\n\n路径\n- 用户级: `~/.nokube/config.yaml` (优先)\n- 全局级: `/etc/.nokube/config.yaml`\n- Agent 容器读取: `/etc/.nokube/config.yaml`（部署时将 `{workspace}/config/config.yaml` 挂载）\n\n格式 (YAML)\n```yaml\netcd_endpoints:\n  - 'http://10.0.0.10:2379'\n  - 'http://10.0.0.11:2379'\n```\n⚠️ 端点必须包含 `http://` 或 `https://` 前缀\n\n读取优先级\n1. 用户级 `~/.nokube/config.yaml`\n2. 全局级 `/etc/.nokube/config.yaml`\n\n配置方法\n- 本机创建上述文件并填入 `etcd_endpoints`\n- 部署时，控制器上传到每节点 `{workspace}/config/config.yaml`\n- 启动 Agent 时绑定为容器内 `/etc/.nokube/config.yaml`\n\n部署分布\n- 本地 CLI/控制器: 读取本机配置\n- 远程节点: `{workspace}/config/config.yaml` → 容器 `/etc/.nokube/config.yaml`（供 Agent/TheProxy 访问 etcd）\n- 相关 etcd 存储键：\n  - ClusterMeta: `cluster/{name}`\n  - ClusterConfig: `{cluster_name}`\n  - Actor: `/nokube/{cluster}/(configmaps|secrets|deployments|daemonsets|pods|events/pod)`","x":-1400,"y":-880,"width":560,"height":980},
		{"id":"config-module","type":"text","text":"## Config 模块\n\n**src/config/**\n- etcd_manager.rs\n- config_manager.rs\n\n🔧 集群配置存储\n📊 节点配置管理","x":-755,"y":-175,"width":260,"height":275},
		{"id":"n-nokube-artifacts","type":"text","text":"## Nokube 打包 (HTTP 分发)\n- 产物目录 (Head 节点): `<workspace>/<mount_subpath>/releases/nokube`\n  - `bin/nokube`\n  - `lib/libcrypto.so.1.1`, `lib/libssl.so.1.1` (可选)\n  - `nokube-image.tar` (可选: 预构建镜像)\n  - `manifest.json`, `install.sh`\n- 生成时机: CLI `new-or-update` 阶段\n- 说明: GitOps Canvas 中 HTTP Server 节点亦有标注","x":-720,"y":-750,"width":520,"height":360},
		{"id":"dash-cluster-home","type":"text","text":"## 监控面板规划\n- Cluster Dashboard 设置为默认首页\n- 顶部新增 ‘关键链接’ 面板，提供快捷跳转：\n  - Actor Dashboard (/d/nokube-actor-dashboard)\n  - Logs (MySQL) (/d/nokube-logs-mysql)\n  - HTTP 文件服务器 (Head 节点)\n  - Greptime Metrics (/v1/prometheus)","x":3870,"y":1720,"width":580,"height":300},
		{"id":"dash-links-panel","type":"text","text":"### 关键链接面板（Cluster 仪表盘内）\n使用 Text 面板 (markdown) 实现：\n- [Actor Dashboard](/d/nokube-actor-dashboard)\n- [Logs (MySQL)](/d/nokube-logs-mysql)\n- [HTTP 文件服务器](http://<head>:<http_port>)\n- [Greptime Metrics](http://<head>:<greptime_port>/v1/prometheus)","x":3870,"y":2085,"width":580,"height":280},
		{"id":"dashboard-logs","type":"text","text":"## Dashboard: Logs (MySQL)\n\n变量\n- container_path: SELECT DISTINCT scope_name AS text FROM opentelemetry_logs WHERE scope_name <> '' ORDER BY text\n\n面板布局\n- Log Messages (Latest):\n\t```\n\tSELECT timestamp AS time, body AS message, severity_text AS level  FROM opentelemetry_logs  WHERE $__timeFilter(timestamp) AND (${container_path:sqlstring} = '' OR scope_name = ${container_path:sqlstring})  ORDER BY timestamp DESC LIMIT 1000\n\t```\n- Log error & warning\n- Log Level Distribution: 按 severity_text 计数 (同样使用 container_path 可选筛选) | Logs per Minute: $__timeGroup(timestamp,'1m') + 可选 container_path 筛选\n\n说明\n- 数据源: `greptimemysql` (Greptime MySQL)\n- 日志面板单行展示：隐藏标签、禁用折叠\n- 使用 scope_name 保存容器 Full Path (cluster/root_actor/pod/container)，用于精确筛选","x":3870,"y":2425,"width":731,"height":575},
		{"id":"dashboard-cluster","type":"text","text":"## Dashboard: Cluster\n\n面板布局\n- 关键链接 | cluster container mem | cluster cpu \n- cluser net rx                    |               cluster net tx \n- each node\n\t- node cpu                 |                   node mem  (both with detail node container usage)\n\n说明\n- 数据源: `GreptimeDB`\n- 时间范围: now-1h；刷新: 15s；采样步长: 15s\n- 心跳: exporter 每 15s 上报一次容器指标\n- Freshness 判定: 近 30s 内未收到新数据则视为下线；用 `count_over_time(metric[30s]) > 0` 过滤活跃容器\n- 主要维度: instance/node, container","x":4521,"y":1720,"width":759,"height":460},
		{"id":"dashboard-actor","type":"text","text":"## Dashboard: Actor\n\n用途\n- 运行归属视角：按 root_actor 分组查看 pod/container 明细与时序\n- 同时包含 Actor 概览（合并原 Service 视图）\n\n变量\n- cluster: `label_values(nokube_container_cpu_cores, cluster_name)`\n- root_actor: `label_values(..., root_actor)`\n- cpu_axis_max, mem_axis_max: 自动探测上限\n\n面板布局\n- 全局总览（合并自 Service）：\n  - Actors Overview (Namespace/Actor/Name/Status/Parent)\n  - Pod–DaemonSet Relationship (status 1/0, stepped)\n  - Container CPU/Memory (%) (stacked)\n  - Service Events Timeline (increase(...[5m]))\n- Root Actors (stat)\n- Container Count ($cluster) (stat)\n- 按 $root_actor 重复分组: \n  - Pods of $root_actor (表: node/pod)\n  - Containers of $root_actor (表: node/pod/container)\n  - Container CPU (cores) [$root_actor] (timeseries)\n  - Container Memory (bytes) [$root_actor] (timeseries)\n\n说明\n- 数据源: `GreptimeDB`\n- 刷新: 15s；采样步长: 15s\n- 心跳: exporter 每 15s 上报一次容器指标\n- Freshness 判定: 近 30s 内未收到新数据则视为下线；面板用 `count_over_time(metric[30s]) > 0` 过滤活跃容器\n- 过滤示例（PromQL/兼容）：\n  - 活跃条件: `count_over_time(nokube_container_cpu[30s]) > 0`\n  - 可选：`max_over_time(nokube_actor_status[30s]) == 1`\n  - Memory(bytes): `max_over_time(nokube_container_mem_bytes[30s])`\n- 仅基于真实容器（已去除演示数据）；无新数据的容器不展示","x":4658,"y":2240,"width":622,"height":1060},
		{"id":"kubecontroller-object-scan-task","type":"text","text":"## KubeController Actor 扫描 [独立task]\n归属: KubeController (start_object_monitor)\n周期: 15s\n职责: 轮询 `/nokube/{cluster}/deployments|daemonsets/`，对比 runtime::deployment::calc_hash_u64 生成的 YAML checksum\n  - 新增/变更: 新 key 或 checksum 变化 → 调用 runtime::deployment::create_deployment_container 重建容器\n  - 心跳缺失/孤儿: Actor alive key 缺失/过期 → 调用 pod.cleanup_if_orphaned() 清理容器与 etcd\n关键代码: src/k8s/controllers.rs::start_object_monitor","x":2140,"y":1560,"width":780,"height":360},
		{"id":"cli-agent-service","type":"text","text":"## CLI: agent-service\n\n用法\n- `nokube agent-service [--extra-params <BASE64(JSON)>]`\n\n说明\n- 常驻服务模式运行 Agent","x":1450,"y":-168,"width":360,"height":362},
		{"id":"exporter-config-task","type":"text","text":"## Exporter 配置轮询 [独立task]\n归属: Exporter\n触发: tokio::spawn (start_config_polling)\n周期: `config_poll_interval` 秒 (默认 10s)\n职责: 读取 `/cluster/{name}` 与 ClusterConfig, 刷新 current_config\n关键代码: src/agent/general/exporter.rs::start_config_polling","x":2100,"y":3300,"width":560,"height":320},
		{"id":"exporter-metrics-task","type":"text","text":"## Exporter 指标采集 [独立task]\n归属: Exporter\n触发: tokio::spawn (start_metrics_collection)\n周期: `metrics_interval` 秒 (默认 30s)\n职责: 聚合系统/容器指标并推送至 GreptimeDB Influx API\n关键代码: src/agent/general/exporter.rs::start_metrics_collection","x":2100,"y":3640,"width":560,"height":320},
		{"id":"exporter-containers","type":"text","text":"## Exporter: 容器指标\n\n目标容器\n- 名称以 `nokube-pod-` 开头（`docker ps --format {{.Names}}` 过滤）\n- 仅来源于真实容器（docker ps + docker stats），不生成演示/模拟数据\n\n采集\n- `docker ps --format \"{{.Names}}\\t{{.Status}}\"` 获取容器名/状态\n- `docker stats --no-stream --format \"{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.MemPerc}}\"` 获取 CPU 与内存\n\n心跳与 Freshness\n- 心跳: 每 15s 推送一次\n- Freshness: 面板用 `count_over_time(metric[30s]) > 0` 过滤，仅展示近 30s 活跃容器\n\n指标名与别名\n- `nokube_container_cpu_usage` → 别名 `nokube_container_cpu`（面板统一使用）\n- `nokube_container_mem_bytes`, `nokube_container_mem_percent`\n- Actor指标: `nokube_actor_info`, `nokube_actor_pod_status`, `nokube_actor_status`\n\n标签\n- 基本: `cluster_name`, `node`, `instance`, `container`, `container_name`\n- Pod/Actor 归属: `pod`, `pod_name`, `parent_deployment`/`parent_daemonset`, `root_actor`\n- 统一路径: `container_path=cluster/root_actor/pod/container`（与日志 `scope_name` 对齐）\n- 归一化: 由容器名推断 core，`upper_actor = <core>-<cluster>`\n","x":2090,"y":3905,"width":820,"height":580},
		{"id":"exporter-system","type":"text","text":"## Exporter: 系统指标\n\n采集\n- CPU 使用率: `/proc/stat`\n- CPU 逻辑核: `/proc/cpuinfo`（或 `/sys/devices/system/cpu/present` 兜底）\n- 内存占用: `/proc/meminfo` (MemTotal/MemAvailable)\n- 网络流量: `/proc/net/dev`（汇总非 `lo` 接口）\n\n指标名\n- `nokube_cpu_usage`, `nokube_cpu_cores`\n- `nokube_memory_usage`, `nokube_memory_used_bytes`, `nokube_memory_total_bytes`\n- `nokube_network_rx_bytes`, `nokube_network_tx_bytes`\n\n周期/新鲜度\n- Agent 内置 MetricsCollector: 推送间隔 15s（容器/Actor指标心跳）\n- Rust Exporter（exporter.rs）默认 30s（系统与容器聚合指标）；面板统一以 30s Freshness 过滤\n\n标签\n- `cluster_name`, `node`, `instance`（三者取自节点/集群上下文）","x":2090,"y":3300,"width":820,"height":560},
		{"id":"21029617ed1cf87d","type":"text","text":"","x":3840,"y":1700,"width":250,"height":4},
		{"id":"n-fix-workspace","type":"text","text":"## ConfigMap 挂载实现（细化）\n- 工作目录来源：从 ClusterConfig 读取当前节点 workspace（避免硬编码）。\n- 写入：将 ConfigMap data 写入 `{workspace}/configmaps/{deployment}` 目录，若不存在先创建。\n- 挂载：通过 DockerRunConfig.add_volume 将上述目录以只读方式挂载到容器 `/etc/config`。\n- 生效面：Deployment/DaemonSet 统一使用节点 workspace，KubeController 初始化亦保持一致。\n- 相关文件：src/agent/service_agent/service_mode_agent.rs（统一容器创建逻辑）","x":2940,"y":1450,"width":700,"height":360},
		{"id":"cli-logs","type":"text","text":"## CLI: logs\n\n用法\n- `nokube logs <pod> [-c <name>] [--follow] [--tail N]`\n\n说明\n- 结合 etcd 状态/事件；Running 时展示示例日志；支持 `--follow`\n\n示例\n- `nokube logs my-pod -c home-cluster --tail 100`","x":1900,"y":-174,"width":360,"height":362},
		{"id":"cli-describe","type":"text","text":"## CLI: describe\n\n用法\n- `nokube describe <pod|deployment|daemonset|configmap|secret|service> <name> [-c <name>]`\n\n说明\n- 从 etcd 或内置渲染输出资源详情\n\n示例\n- `nokube describe pod gitops-controller-0 -c home-cluster`","x":1900,"y":-577,"width":360,"height":362},
		{"id":"cli-agent-command","type":"text","text":"## CLI: agent-command\n\n用法\n- `nokube agent-command --extra-params <BASE64(JSON)>`\n\n说明\n- 一次性命令模式；`extra-params` 必须包含 `cluster_name`\n\n示例\n- `echo -n '{\"cluster_name\":\"home-cluster\"}' | base64`","x":1450,"y":-571,"width":360,"height":362},
		{"id":"cli-apply","type":"text","text":"## CLI: apply\n\n用法\n- `nokube apply -f <file> [--cluster <name>] [--dry-run]`\n- `cat manifest.yaml | nokube apply --cluster <name>`\n\n说明\n- 支持多文档 YAML；未指定 `-f` 时从 stdin 读取\n- `--dry-run` 仅解析/打印，不写入 etcd\n\n示例\n- `nokube apply -f gitops.yaml --cluster home-cluster`","x":2280,"y":-577,"width":360,"height":362},
		{"id":"cli-get","type":"text","text":"## CLI: get\n\n用法\n- `nokube get <pods|deployments|daemonsets|configmaps|secrets> [name] [-o table|json|yaml] [-c <name>] [--all-namespaces]`\n\n说明\n- 默认输出 `-o table`；`-c` 等同 `--cluster`\n- services 列表暂未实现\n\n示例\n- `nokube get pods -c home-cluster -o table`","x":2280,"y":-174,"width":360,"height":362},
		{"id":"n-hostport","type":"text","text":"## Decision: hostPort vs Service\n- 配置 hostPort 后，Pod 在节点 NodeIP:hostPort 直接可达，不依赖 Service。\n- 仍建议：为稳定发现/DNS 与负载均衡叠加 ClusterIP Service。\n- 跨节点对外访问：用 NodePort/LoadBalancer（需要 Service）。\n- 代价：同节点端口冲突、调度受限。","x":2810,"y":-620,"width":520,"height":300},
		{"id":"n-ports","type":"text","text":"## Ports/hostPort 支持（TODO）\n- 解析 spec.template.spec.containers[].ports/hostPort → DockerRunConfig.add_port()。\n- 如需 Service 能力：实现 apply_service 持久化 + get/describe service。\n- 策略：hostPort 用于本机暴露；Service 提供发现与负载。","x":2835,"y":-293,"width":520,"height":300},
		{"id":"kubecontroller-overview","type":"text","text":"## KubeController（中心回收）\n\n- 归属路径: `service/{node}/kubecontroller`（由 ServiceModeAgent 启动）\n- 共享 TheProxy 发送端，负责 actor 心跳续约、孤儿回收\n- 每 15s 调度巡检：遍历 deployments/daemonsets，触发 pod.cleanup_if_orphaned()\n- Actor扫描: 轮询 `/nokube/{cluster}/deployments|daemonsets/`，直接构建/重建容器\n- 与 Grafana Freshness 对齐：确保 30s 内无心跳的容器全部被清空","x":1250,"y":1500,"width":820,"height":620},
		{"id":"actor-todos","type":"text","text":"## Actor TODO / Roadmap\n- Alive 采用 etcd 租约（lease），替换仅写 TTL 字段\n- 孤儿回收下沉：由各 actor 自检自清理\n- 版本更新：对 etcd 值计算 checksum→ 变更即优雅重建\n- 端口支持：解析 spec.containers[].ports/hostPort → DockerRunConfig.add_port()\n- 状态回写：pods / events 持续更新错误信息\n- 观测一致性：日志 scope_name 与指标 container_path 一致\n","x":1290,"y":2160,"width":560,"height":360},
		{"id":"n-gitops-note","type":"text","text":"## GitOps Controller 部署要点\n- 镜像：python:3.10-slim；容器名：nokube-pod-<deployment>。\n- ConfigMap：gitops-scripts-<cluster> 挂载至 /etc/config（requirements.txt, gitops-controller.py）。\n- 命令：pip install -r /etc/config/requirements.txt && python /etc/config/gitops-controller.py。\n- 真实状态以 etcd /pods 与容器为准。","x":1020,"y":1140,"width":520,"height":320},
		{"id":"63020bf5856229b9","type":"text","text":"## Exporter 总览\n\n路径\n- `src/agent/general/exporter.rs`\n\n职责\n- 采集系统与容器指标，推送到 GreptimeDB (Influx 行协议)\n- 按配置轮询 etcd 获取 `ClusterConfig`\n- 仅基于真实容器（docker ps + docker stats），不生成演示/模拟数据\n\n推送目标\n- `{greptime}/v1/influxdb/write`（由 head 节点 IP + `monitoring.greptimedb.port` 组装）\n\n周期\n- metrics_interval 默认 30s\n- config_poll_interval 默认 10s\n- Agent MetricsCollector(容器/Actor) 心跳 15s；面板统一 30s Freshness 过滤（`count_over_time(metric[30s]) > 0`）\n\n指标与别名\n- CPU: `nokube_container_cpu_usage` → 别名 `nokube_container_cpu`（面板统一使用）\n- 内存: `nokube_container_mem_bytes`, `nokube_container_mem_percent`\n- Actor指标: `nokube_actor_info`, `nokube_actor_pod_status`, `nokube_actor_status`\n\n标签增强\n- 容器指标追加 `container_path=cluster/root_actor/pod/container`，与日志 `scope_name` 对齐，便于仪表盘联动\n\n支持\n- Cluster / Actor 仪表盘\n","x":1530,"y":3300,"width":530,"height":680},
		{"id":"actor-comm","type":"text","text":"## ServiceAgent 间通信\n\n### 首选: etcd 作为控制面总线\n- 事件/信号/心跳均走 etcd 前缀 `/nokube/{cluster}/actors/**`\n- 优点: 去中心、幂等、天然多节点\n\n### 一致性约束\n- 所有操作需幂等，并通过 etcd 可重试","x":3080,"y":1970,"width":560,"height":520},
		{"id":"root-actor-definition","type":"text","text":"## Root Actor 概念\n- 定义: Actor 层级的根节点，用于聚合 Pod/容器指标与日志\n- 分类: deployment/<name> 或 daemonset/<name>\n- 生成: 创建 Deployment/DaemonSet Actor 时自动生成顶层路径\n- 作用: root_actor 标签指导 Grafana 仪表盘与日志聚合","x":2170,"y":2030,"width":600,"height":260}
	],
	"edges":[
		{"id":"edge-1","fromNode":"nokube-core","fromSide":"left","toNode":"config-module","toSide":"right","label":"配置读取"},
		{"id":"edge-2","fromNode":"nokube-core","fromSide":"bottom","toNode":"agent-system","toSide":"top","label":"代理管理"},
		{"id":"edge-3","fromNode":"nokube-core","fromSide":"right","toNode":"remote-control","toSide":"left","label":"远程部署"},
		{"id":"edge-5","fromNode":"agent-system","fromSide":"bottom","toNode":"30cdd7fa9b891781","toSide":"left"},
		{"id":"edge-service-etcd","fromNode":"k8s-abstraction","fromSide":"left","toNode":"etcd-storage","toSide":"right","label":"读取配置，部署actor"},
		{"id":"edge-cli-etcd","fromNode":"nokube-core","fromSide":"left","toNode":"etcd-storage","toSide":"right","label":"查询Pod状态"},
		{"id":"edge-docker-ops","fromNode":"8c4790e1a4bce75f","fromSide":"right","toNode":"docker-ops-module","toSide":"left","label":"容器管理"},
		{"id":"edge-docker-k8s","fromNode":"k8s-abstraction","fromSide":"right","toNode":"docker-ops-module","toSide":"left","label":"Pod容器化"},
		{"id":"edge-docker-example","fromNode":"docker-ops-module","fromSide":"bottom","toNode":"docker-config-example","toSide":"bottom","label":"实现示例"},
		{"id":"edge-config-workspace","fromNode":"cluster-config","fromSide":"right","toNode":"docker-config-example","toSide":"bottom","label":"{workspace}配置来源"},
		{"id":"edge-config-etcd","fromNode":"cluster-config","fromSide":"top","toNode":"etcd-storage","toSide":"bottom","label":"集群配置存储"},
		{"id":"edge-config-docker","fromNode":"cluster-config","fromSide":"right","toNode":"docker-ops-module","toSide":"left","label":"workspace路径"},
		{"id":"03a2ca3d55d61bc6","fromNode":"remote-control","fromSide":"bottom","toNode":"9213010bd51b4eba","toSide":"top"},
		{"id":"151253ad588032ae","fromNode":"config-module","fromSide":"bottom","toNode":"029a051cde4f5915","toSide":"top"},
		{"id":"c7f595250ae29636","fromNode":"nokube-config","fromSide":"right","toNode":"config-module","toSide":"left"},
		{"id":"edge-gitops-note","fromNode":"k8s-abstraction","fromSide":"right","toNode":"actor-lifecycle","toSide":"left"},
		{"id":"edge-fix-configmap","fromNode":"docker-config-example","fromSide":"bottom","toNode":"n-fix-workspace","toSide":"top","label":"ConfigMap 挂载细化"},
		{"id":"edge-actor-todos","fromNode":"actor-lifecycle","fromSide":"right","toNode":"actor-todos","toSide":"left","label":"actor 待办"},
		{"id":"f97ef2b236dddf75","fromNode":"exporter-containers","fromSide":"bottom","toNode":"dashboard-actor","toSide":"bottom","color":"3"},
		{"id":"8ea4fa6add7df724","fromNode":"actor-lifecycle","fromSide":"right","toNode":"kubecontroller-overview","toSide":"left"},
		{"id":"edge-exporter-config-task","fromNode":"63020bf5856229b9","fromSide":"right","toNode":"exporter-config-task","toSide":"left","label":"spawn"},
		{"id":"edge-exporter-metrics-task","fromNode":"63020bf5856229b9","fromSide":"right","toNode":"exporter-metrics-task","toSide":"left","label":"spawn"},
		{"id":"edge-theproxy-alive","fromNode":"63a7919ba26e1c77","fromSide":"right","toNode":"the-proxy-alive-task","toSide":"left","color":"2","label":"tokio::spawn"},
		{"id":"edge-theproxy-keepalive","fromNode":"63a7919ba26e1c77","fromSide":"right","toNode":"the-proxy-keepalive-task","toSide":"left","color":"2","label":"tokio::spawn"},
		{"id":"edge-kubecontroller-object-task","fromNode":"kubecontroller-overview","fromSide":"right","toNode":"kubecontroller-object-scan-task","toSide":"left","color":"2","label":"tokio::spawn"}
	]
}